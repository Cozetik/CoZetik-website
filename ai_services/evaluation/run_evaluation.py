#!/usr/bin/env python3
"""
evaluation/run_evaluation.py

Pipeline d'√©valuation NLP pour mesurer la qualit√© des recommandations.
Utilise:
- Sentence-Transformers pour les embeddings
- Cosine Similarity pour comparer les sorties
- Matplotlib pour la visualisation
"""

import os
import sys
import json
import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple

# Ajouter le path parent pour les imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import httpx
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

from evaluation.test_cases import TEST_CASES

# Configuration
API_URL = "http://127.0.0.1:8000/api/recommander"
EMBEDDING_MODEL = "paraphrase-multilingual-MiniLM-L12-v2"  # Mod√®le multilingue rapide


def call_api(answers: Dict[str, str]) -> Dict:
    """Appelle l'API de recommandation."""
    response = httpx.post(API_URL, json={"answers": answers}, timeout=60.0)
    response.raise_for_status()
    return response.json()


def calculate_program_match(actual: str, expected: str) -> float:
    """
    V√©rifie si le programme recommand√© correspond √† l'attendu.
    Retourne 1.0 si match, 0.0 sinon.
    """
    actual_lower = actual.lower()
    expected_lower = expected.lower()
    
    # Match exact ou partiel
    if expected_lower in actual_lower or actual_lower in expected_lower:
        return 1.0
    return 0.0


def calculate_keyword_coverage(text: str, keywords: List[str]) -> float:
    """
    Calcule le pourcentage de mots-cl√©s attendus pr√©sents dans le texte.
    """
    text_lower = text.lower()
    matches = sum(1 for kw in keywords if kw.lower() in text_lower)
    return matches / len(keywords) if keywords else 0.0


def calculate_semantic_similarity(
    model: SentenceTransformer,
    actual_text: str,
    expected_keywords: List[str]
) -> float:
    """
    Calcule la similarit√© cosinus entre le texte g√©n√©r√© et les mots-cl√©s attendus.
    """
    # Cr√©er une phrase repr√©sentative avec les mots-cl√©s
    expected_text = " ".join(expected_keywords)
    
    # G√©n√©rer les embeddings
    embeddings = model.encode([actual_text, expected_text])
    
    # Calculer la similarit√© cosinus
    similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
    
    return float(similarity)


def run_evaluation() -> List[Dict]:
    """
    Ex√©cute l'√©valuation compl√®te sur tous les cas de test.
    """
    print("=" * 60)
    print("üî¨ √âVALUATION NLP DES RECOMMANDATIONS COZETIK")
    print("=" * 60)
    
    # Charger le mod√®le d'embeddings
    print("\nüì¶ Chargement du mod√®le d'embeddings...")
    model = SentenceTransformer(EMBEDDING_MODEL)
    print(f"   Mod√®le: {EMBEDDING_MODEL}")
    
    results = []
    
    for i, test_case in enumerate(TEST_CASES, 1):
        print(f"\n{'‚îÄ' * 50}")
        print(f"üìù Test {i}/{len(TEST_CASES)}: {test_case['name']}")
        print(f"{'‚îÄ' * 50}")
        
        try:
            # Appeler l'API
            print("   ‚è≥ Appel API...")
            response = call_api(test_case["answers"])
            
            # Extraire les donn√©es
            actual_program = response.get("principal_program", {}).get("name", "")
            actual_reason = response.get("principal_program", {}).get("reason", "")
            actual_analysis = response.get("profil_analysis", "")
            actual_motivation = response.get("motivation_message", "")
            
            # Texte complet pour l'analyse s√©mantique
            full_text = f"{actual_analysis} {actual_reason} {actual_motivation}"
            
            # Calculer les m√©triques
            program_match = calculate_program_match(actual_program, test_case["expected_program"])
            keyword_coverage = calculate_keyword_coverage(full_text, test_case["expected_keywords"])
            semantic_sim = calculate_semantic_similarity(model, full_text, test_case["expected_keywords"])
            
            # Score global (moyenne pond√©r√©e)
            global_score = (program_match * 0.5) + (keyword_coverage * 0.25) + (semantic_sim * 0.25)
            
            result = {
                "test_name": test_case["name"],
                "expected_program": test_case["expected_program"],
                "actual_program": actual_program,
                "program_match": program_match,
                "keyword_coverage": keyword_coverage,
                "semantic_similarity": semantic_sim,
                "global_score": global_score,
                "full_response": response
            }
            results.append(result)
            
            # Affichage
            match_emoji = "‚úÖ" if program_match == 1.0 else "‚ùå"
            print(f"   Programme attendu: {test_case['expected_program']}")
            print(f"   Programme obtenu:  {actual_program} {match_emoji}")
            print(f"   üìä Programme Match:      {program_match:.0%}")
            print(f"   üìä Keyword Coverage:     {keyword_coverage:.0%}")
            print(f"   üìä Similarit√© Cosinus:   {semantic_sim:.2%}")
            print(f"   üìä SCORE GLOBAL:         {global_score:.2%}")
            
        except Exception as e:
            print(f"   ‚ùå Erreur: {str(e)}")
            results.append({
                "test_name": test_case["name"],
                "error": str(e),
                "global_score": 0.0
            })
    
    return results


def generate_visualizations(results: List[Dict]):
    """
    G√©n√®re les graphiques de visualisation.
    Cr√©e 4 images s√©par√©es avec l√©gendes et explications.
    """
    print("\n" + "=" * 60)
    print("üìä G√âN√âRATION DES VISUALISATIONS")
    print("=" * 60)
    
    # Filtrer les r√©sultats sans erreur
    valid_results = [r for r in results if "error" not in r]
    
    if not valid_results:
        print("   ‚ùå Aucun r√©sultat valide √† visualiser.")
        return
    
    # Dossier de sortie
    output_dir = os.path.dirname(__file__)
    
    # Donn√©es pour les graphiques
    test_names = [r["test_name"].replace(" - ", "\n") for r in valid_results]
    short_names = [r["test_name"].split(" - ")[0] for r in valid_results]
    program_matches = [r["program_match"] for r in valid_results]
    keyword_coverages = [r["keyword_coverage"] for r in valid_results]
    semantic_sims = [r["semantic_similarity"] for r in valid_results]
    global_scores = [r["global_score"] for r in valid_results]
    
    # Couleurs coh√©rentes
    colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(valid_results)))
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # GRAPHIQUE 1: Score Global par Profil (Barres horizontales)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    fig1, ax1 = plt.subplots(figsize=(12, 7))
    
    bars = ax1.barh(test_names, global_scores, color=colors, edgecolor="black", linewidth=0.5)
    ax1.set_xlim(0, 1.15)
    ax1.set_xlabel("Score Global (0 √† 1)", fontsize=12)
    ax1.set_title("üéØ SCORE GLOBAL PAR PROFIL DE TEST", fontsize=14, fontweight="bold", pad=20)
    
    # Seuil acceptable
    ax1.axvline(x=0.8, color="green", linestyle="--", linewidth=2, alpha=0.8, label="Seuil acceptable (80%)")
    ax1.axvline(x=0.6, color="orange", linestyle="--", linewidth=1.5, alpha=0.6, label="Seuil minimum (60%)")
    
    # Valeurs sur les barres
    for bar, score in zip(bars, global_scores):
        color = "green" if score >= 0.8 else ("orange" if score >= 0.6 else "red")
        ax1.text(bar.get_width() + 0.02, bar.get_y() + bar.get_height()/2, 
                 f"{score:.0%}", va="center", fontsize=11, fontweight="bold", color=color)
    
    ax1.legend(loc="lower right", fontsize=10)
    
    # Explication
    explanation = """
    üìå INTERPR√âTATION:
    ‚Ä¢ Score Global = (Programme Match √ó 50%) + (Keyword Coverage √ó 25%) + (Similarit√© Cosinus √ó 25%)
    ‚Ä¢ ‚â• 80%: EXCELLENT - Le mod√®le recommande correctement
    ‚Ä¢ 60-80%: BON - Performance acceptable
    ‚Ä¢ < 60%: √Ä AM√âLIORER
    """
    fig1.text(0.02, 0.02, explanation, fontsize=9, verticalalignment="bottom", 
              bbox=dict(boxstyle="round", facecolor="lightyellow", alpha=0.8))
    
    plt.tight_layout()
    path1 = os.path.join(output_dir, "chart_1_score_global.png")
    plt.savefig(path1, dpi=150, bbox_inches="tight")
    print(f"   ‚úÖ {path1}")
    plt.close()
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # GRAPHIQUE 2: D√©tail des M√©triques (Barres group√©es)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    fig2, ax2 = plt.subplots(figsize=(12, 7))
    
    x = np.arange(len(short_names))
    width = 0.25
    
    bars1 = ax2.bar(x - width, program_matches, width, label="Programme Match", color="#2ecc71", edgecolor="black")
    bars2 = ax2.bar(x, keyword_coverages, width, label="Keyword Coverage", color="#3498db", edgecolor="black")
    bars3 = ax2.bar(x + width, semantic_sims, width, label="Similarit√© Cosinus", color="#9b59b6", edgecolor="black")
    
    ax2.set_ylabel("Score (0 √† 1)", fontsize=12)
    ax2.set_title("üìä D√âTAIL DES 3 M√âTRIQUES PAR PROFIL", fontsize=14, fontweight="bold", pad=20)
    ax2.set_xticks(x)
    ax2.set_xticklabels(short_names, rotation=30, ha="right", fontsize=10)
    ax2.legend(loc="upper right", fontsize=10)
    ax2.set_ylim(0, 1.2)
    ax2.axhline(y=1.0, color="gray", linestyle=":", alpha=0.5)
    
    # Grille
    ax2.yaxis.grid(True, linestyle="--", alpha=0.3)
    ax2.set_axisbelow(True)
    
    # Explication
    explanation = """
    üìå LES 3 M√âTRIQUES:
    ‚Ä¢ Programme Match (Vert): Le bon programme signature a-t-il √©t√© recommand√©? (1 = OUI, 0 = NON)
    ‚Ä¢ Keyword Coverage (Bleu): % de mots-cl√©s attendus pr√©sents dans la r√©ponse (ex: "productivit√©", "temps"...)
    ‚Ä¢ Similarit√© Cosinus (Violet): Proximit√© s√©mantique entre la r√©ponse et les attentes (embeddings NLP)
    """
    fig2.text(0.02, 0.02, explanation, fontsize=9, verticalalignment="bottom", 
              bbox=dict(boxstyle="round", facecolor="lightyellow", alpha=0.8))
    
    plt.tight_layout()
    path2 = os.path.join(output_dir, "chart_2_metriques_detail.png")
    plt.savefig(path2, dpi=150, bbox_inches="tight")
    print(f"   ‚úÖ {path2}")
    plt.close()
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # GRAPHIQUE 3: Radar Chart (Moyennes Globales)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    fig3 = plt.figure(figsize=(10, 8))
    ax3 = fig3.add_subplot(111, projection="polar")
    
    categories = ["Programme\nMatch", "Keyword\nCoverage", "Similarit√©\nCosinus"]
    
    # Moyenne des scores
    avg_program = np.mean(program_matches)
    avg_keywords = np.mean(keyword_coverages)
    avg_semantic = np.mean(semantic_sims)
    
    values = [avg_program, avg_keywords, avg_semantic]
    values += values[:1]  # Fermer le polygone
    
    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
    angles += angles[:1]
    
    ax3.set_theta_offset(np.pi / 2)
    ax3.set_theta_direction(-1)
    ax3.plot(angles, values, "o-", linewidth=3, color="#e74c3c", markersize=10)
    ax3.fill(angles, values, alpha=0.3, color="#e74c3c")
    ax3.set_xticks(angles[:-1])
    ax3.set_xticklabels(categories, fontsize=12)
    ax3.set_ylim(0, 1)
    ax3.set_title("üï∏Ô∏è MOYENNES GLOBALES (RADAR CHART)", fontsize=14, fontweight="bold", pad=30)
    
    # Ajouter les valeurs
    for angle, value, cat in zip(angles[:-1], values[:-1], categories):
        ax3.annotate(f"{value:.0%}", xy=(angle, value), xytext=(angle, value + 0.12),
                     ha="center", fontsize=11, fontweight="bold", color="#e74c3c")
    
    # Explication
    explanation = f"""
    üìå INTERPR√âTATION DU RADAR:
    ‚Ä¢ Programme Match Moyen: {avg_program:.0%}
    ‚Ä¢ Keyword Coverage Moyen: {avg_keywords:.0%}  
    ‚Ä¢ Similarit√© Cosinus Moy.: {avg_semantic:.0%}
    
    ‚û°Ô∏è Plus le triangle est grand et √©quilibr√©, meilleure est la performance.
    """
    fig3.text(0.5, 0.02, explanation, fontsize=10, ha="center", 
              bbox=dict(boxstyle="round", facecolor="lightyellow", alpha=0.8))
    
    plt.tight_layout()
    path3 = os.path.join(output_dir, "chart_3_radar_moyennes.png")
    plt.savefig(path3, dpi=150, bbox_inches="tight")
    print(f"   ‚úÖ {path3}")
    plt.close()
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # GRAPHIQUE 4: Heatmap des Scores
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    fig4, ax4 = plt.subplots(figsize=(12, 6))
    
    data = np.array([program_matches, keyword_coverages, semantic_sims])
    im = ax4.imshow(data, cmap="RdYlGn", aspect="auto", vmin=0, vmax=1)
    
    ax4.set_yticks([0, 1, 2])
    ax4.set_yticklabels(["Programme Match", "Keyword Coverage", "Similarit√© Cosinus"], fontsize=11)
    ax4.set_xticks(range(len(short_names)))
    ax4.set_xticklabels(short_names, rotation=30, ha="right", fontsize=10)
    ax4.set_title("üå°Ô∏è HEATMAP DES SCORES PAR PROFIL ET M√âTRIQUE", fontsize=14, fontweight="bold", pad=20)
    
    # Colorbar
    cbar = plt.colorbar(im, ax=ax4, shrink=0.8)
    cbar.set_label("Score (0 = Rouge, 1 = Vert)", fontsize=10)
    
    # Valeurs dans les cellules
    for i in range(data.shape[0]):
        for j in range(data.shape[1]):
            text_color = "white" if data[i, j] < 0.5 else "black"
            ax4.text(j, i, f"{data[i, j]:.0%}", ha="center", va="center", 
                     color=text_color, fontsize=11, fontweight="bold")
    
    # Explication
    explanation = """
    üìå LECTURE DE LA HEATMAP:
    ‚Ä¢ Vert = Score √©lev√© (proche de 100%)
    ‚Ä¢ Rouge = Score faible (proche de 0%)
    ‚Ä¢ Permet d'identifier rapidement les faiblesses par profil et par m√©trique
    """
    fig4.text(0.02, 0.02, explanation, fontsize=9, verticalalignment="bottom", 
              bbox=dict(boxstyle="round", facecolor="lightyellow", alpha=0.8))
    
    plt.tight_layout()
    path4 = os.path.join(output_dir, "chart_4_heatmap.png")
    plt.savefig(path4, dpi=150, bbox_inches="tight")
    print(f"   ‚úÖ {path4}")
    plt.close()
    
    print(f"\n   üìÅ Tous les graphiques sont dans: {output_dir}/")


def print_summary(results: List[Dict]):
    """
    Affiche un r√©sum√© des r√©sultats.
    """
    print("\n" + "=" * 60)
    print("üìã R√âSUM√â DE L'√âVALUATION")
    print("=" * 60)
    
    valid_results = [r for r in results if "error" not in r]
    
    if not valid_results:
        print("   ‚ùå Aucun test r√©ussi.")
        return
    
    avg_global = np.mean([r["global_score"] for r in valid_results])
    avg_program = np.mean([r["program_match"] for r in valid_results])
    avg_keywords = np.mean([r["keyword_coverage"] for r in valid_results])
    avg_semantic = np.mean([r["semantic_similarity"] for r in valid_results])
    
    print(f"\n   üìä Score Global Moyen:       {avg_global:.1%}")
    print(f"   üìä Accuracy Programme:       {avg_program:.1%}")
    print(f"   üìä Keyword Coverage Moyen:   {avg_keywords:.1%}")
    print(f"   üìä Similarit√© Cosinus Moy.:  {avg_semantic:.1%}")
    
    # Verdict
    print("\n   " + "‚îÄ" * 40)
    if avg_global >= 0.8:
        print("   üèÜ VERDICT: EXCELLENT - Le mod√®le performe tr√®s bien!")
    elif avg_global >= 0.6:
        print("   ‚úÖ VERDICT: BON - Performance acceptable.")
    elif avg_global >= 0.4:
        print("   ‚ö†Ô∏è  VERDICT: MOYEN - Am√©lioration n√©cessaire.")
    else:
        print("   ‚ùå VERDICT: FAIBLE - Le mod√®le doit √™tre revu.")
    print("   " + "‚îÄ" * 40)


if __name__ == "__main__":
    print("\nüöÄ D√©marrage de l'√©valuation...")
    print("   ‚ö†Ô∏è  Assurez-vous que le serveur uvicorn est lanc√©!")
    print()
    
    # Lancer l'√©valuation
    results = run_evaluation()
    
    # Afficher le r√©sum√©
    print_summary(results)
    
    # G√©n√©rer les visualisations
    generate_visualizations(results)
    
    # Sauvegarder les r√©sultats JSON
    output_json = os.path.join(os.path.dirname(__file__), "evaluation_results.json")
    with open(output_json, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"\n   ‚úÖ R√©sultats JSON: {output_json}")
    
    print("\nüéâ √âvaluation termin√©e!")
